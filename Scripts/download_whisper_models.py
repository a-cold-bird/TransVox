#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Whisper æ¨¡å‹ä¸‹è½½è„šæœ¬
ç”¨äºé¢„å…ˆä¸‹è½½ faster-whisper æ¨¡å‹åˆ°æœ¬åœ°ï¼Œé¿å…è¿è¡Œæ—¶ä¸‹è½½
"""

import os
import sys
import argparse
from pathlib import Path
from faster_whisper import WhisperModel
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨
AVAILABLE_MODELS = {
    'tiny': 'Systran/faster-whisper-tiny',
    'tiny.en': 'Systran/faster-whisper-tiny.en',
    'base': 'Systran/faster-whisper-base',
    'base.en': 'Systran/faster-whisper-base.en',
    'small': 'Systran/faster-whisper-small',
    'small.en': 'Systran/faster-whisper-small.en',
    'medium': 'Systran/faster-whisper-medium',
    'medium.en': 'Systran/faster-whisper-medium.en',
    'large-v1': 'Systran/faster-whisper-large-v1',
    'large-v2': 'Systran/faster-whisper-large-v2',
    'large-v3': 'Systran/faster-whisper-large-v3',
}

# æ¨¡å‹å¤§å°ä¿¡æ¯ï¼ˆè¿‘ä¼¼ï¼‰
MODEL_SIZES = {
    'tiny': '~75 MB',
    'tiny.en': '~75 MB',
    'base': '~142 MB',
    'base.en': '~142 MB',
    'small': '~466 MB',
    'small.en': '~466 MB',
    'medium': '~1.5 GB',
    'medium.en': '~1.5 GB',
    'large-v1': '~2.9 GB',
    'large-v2': '~2.9 GB',
    'large-v3': '~2.9 GB',
}


def download_model(model_name: str, device: str = "cpu", compute_type: str = "int8",
                   cache_dir: str = None) -> bool:
    """
    ä¸‹è½½æŒ‡å®šçš„ Whisper æ¨¡å‹

    Args:
        model_name: æ¨¡å‹åç§°ï¼ˆå¦‚ 'base', 'large-v3'ï¼‰
        device: è®¾å¤‡ç±»å‹ï¼ˆ'cpu' æˆ– 'cuda'ï¼‰
        compute_type: è®¡ç®—ç±»å‹ï¼ˆ'int8', 'float16', 'float32'ï¼‰
        cache_dir: ç¼“å­˜ç›®å½•ï¼ˆå¯é€‰ï¼‰

    Returns:
        bool: ä¸‹è½½æ˜¯å¦æˆåŠŸ
    """
    if model_name not in AVAILABLE_MODELS:
        logger.error(f"âŒ ä¸æ”¯æŒçš„æ¨¡å‹: {model_name}")
        logger.info(f"å¯ç”¨æ¨¡å‹: {', '.join(AVAILABLE_MODELS.keys())}")
        return False

    repo_id = AVAILABLE_MODELS[model_name]
    logger.info(f"ğŸ“¥ å¼€å§‹ä¸‹è½½æ¨¡å‹: {model_name}")
    logger.info(f"   HuggingFace Repo: {repo_id}")
    logger.info(f"   é¢„è®¡å¤§å°: {MODEL_SIZES.get(model_name, 'æœªçŸ¥')}")
    logger.info(f"   è®¾å¤‡: {device}")
    logger.info(f"   è®¡ç®—ç±»å‹: {compute_type}")

    if cache_dir:
        logger.info(f"   ç¼“å­˜ç›®å½•: {cache_dir}")

    try:
        # ä¸‹è½½å¹¶åˆå§‹åŒ–æ¨¡å‹ï¼ˆè¿™ä¼šè§¦å‘æ¨¡å‹ä¸‹è½½ï¼‰
        logger.info("â³ æ­£åœ¨ä¸‹è½½æ¨¡å‹æ–‡ä»¶...")

        kwargs = {
            'device': device,
            'compute_type': compute_type,
        }

        if cache_dir:
            kwargs['download_root'] = cache_dir

        model = WhisperModel(repo_id, **kwargs)

        logger.info(f"âœ… æ¨¡å‹ä¸‹è½½æˆåŠŸ: {model_name}")

        # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
        logger.info(f"   æ¨¡å‹å·²ç¼“å­˜ï¼Œåç»­ä½¿ç”¨å°†ç›´æ¥ä»æœ¬åœ°åŠ è½½")

        return True

    except Exception as e:
        logger.error(f"âŒ æ¨¡å‹ä¸‹è½½å¤±è´¥: {e}")
        return False


def list_models():
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ¨¡å‹"""
    print("\nå¯ç”¨çš„ Whisper æ¨¡å‹:")
    print("=" * 70)
    print(f"{'æ¨¡å‹åç§°':<15} {'å¤§å°':<15} {'HuggingFace Repo'}")
    print("-" * 70)
    for name, repo in AVAILABLE_MODELS.items():
        size = MODEL_SIZES.get(name, 'æœªçŸ¥')
        print(f"{name:<15} {size:<15} {repo}")
    print("=" * 70)
    print("\næ¨è:")
    print("  - å¿«é€Ÿæµ‹è¯•: tiny, base")
    print("  - å¹³è¡¡æ€§èƒ½: small, medium")
    print("  - æœ€ä½³è´¨é‡: large-v3 (æ¨è)")
    print("  - ä»…è‹±è¯­: é€‰æ‹© .en åç¼€çš„æ¨¡å‹")
    print()


def main():
    parser = argparse.ArgumentParser(
        description="Whisper æ¨¡å‹ä¸‹è½½å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # ä¸‹è½½ base æ¨¡å‹ï¼ˆCPUï¼Œæ¨èç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰
  python download_whisper_models.py base

  # ä¸‹è½½ large-v3 æ¨¡å‹ï¼ˆGPUï¼Œæ¨èç”¨äºç”Ÿäº§ç¯å¢ƒï¼‰
  python download_whisper_models.py large-v3 --device cuda --compute-type float16

  # ä¸‹è½½å¤šä¸ªæ¨¡å‹
  python download_whisper_models.py base small large-v3

  # åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ¨¡å‹
  python download_whisper_models.py --list
        """
    )

    parser.add_argument(
        'models',
        nargs='*',
        help='è¦ä¸‹è½½çš„æ¨¡å‹åç§°ï¼ˆå¯ä»¥æŒ‡å®šå¤šä¸ªï¼‰'
    )

    parser.add_argument(
        '--list',
        action='store_true',
        help='åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ¨¡å‹'
    )

    parser.add_argument(
        '--device',
        choices=['cpu', 'cuda'],
        default='cpu',
        help='è®¾å¤‡ç±»å‹ï¼ˆé»˜è®¤: cpuï¼‰'
    )

    parser.add_argument(
        '--compute-type',
        choices=['int8', 'float16', 'float32'],
        default='int8',
        help='è®¡ç®—ç±»å‹ï¼ˆé»˜è®¤: int8ï¼Œé€‚åˆ CPUï¼‰'
    )

    parser.add_argument(
        '--cache-dir',
        type=str,
        help='æ¨¡å‹ç¼“å­˜ç›®å½•ï¼ˆå¯é€‰ï¼‰'
    )

    parser.add_argument(
        '--all',
        action='store_true',
        help='ä¸‹è½½æ‰€æœ‰æ¨¡å‹ï¼ˆä¸æ¨èï¼Œæ€»å¤§å°çº¦ 10+ GBï¼‰'
    )

    args = parser.parse_args()

    # åˆ—å‡ºæ¨¡å‹
    if args.list:
        list_models()
        return

    # ç¡®å®šè¦ä¸‹è½½çš„æ¨¡å‹
    models_to_download = []

    if args.all:
        models_to_download = list(AVAILABLE_MODELS.keys())
        logger.warning("âš ï¸  å°†ä¸‹è½½æ‰€æœ‰æ¨¡å‹ï¼Œæ€»å¤§å°çº¦ 10+ GB")
        response = input("ç¡®è®¤ç»§ç»­ï¼Ÿ(yes/no): ")
        if response.lower() not in ['yes', 'y']:
            logger.info("å·²å–æ¶ˆ")
            return
    elif args.models:
        models_to_download = args.models
    else:
        logger.error("âŒ è¯·æŒ‡å®šè¦ä¸‹è½½çš„æ¨¡å‹æˆ–ä½¿ç”¨ --list æŸ¥çœ‹å¯ç”¨æ¨¡å‹")
        parser.print_help()
        return

    # æ£€æŸ¥ CUDA å¯ç”¨æ€§
    import torch
    if args.device == 'cuda':
        if not torch.cuda.is_available():
            logger.warning("âš ï¸  CUDA ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ CPU æ¨¡å¼")
            args.device = 'cpu'
            args.compute_type = 'int8'
        else:
            logger.info(f"âœ… CUDA å¯ç”¨: {torch.cuda.get_device_name(0)}")

    # ä¸‹è½½æ¨¡å‹
    success_count = 0
    failed_models = []

    for model_name in models_to_download:
        print(f"\n{'='*70}")
        success = download_model(
            model_name,
            device=args.device,
            compute_type=args.compute_type,
            cache_dir=args.cache_dir
        )

        if success:
            success_count += 1
        else:
            failed_models.append(model_name)

    # æ€»ç»“
    print(f"\n{'='*70}")
    print(f"ä¸‹è½½å®Œæˆ: {success_count}/{len(models_to_download)} æˆåŠŸ")

    if failed_models:
        print(f"å¤±è´¥çš„æ¨¡å‹: {', '.join(failed_models)}")

    print("=" * 70)


if __name__ == "__main__":
    main()
